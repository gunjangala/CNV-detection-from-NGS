#!/bin/bash
#PBS -V
#PBS -N 100xDEL
#PBS -o /scratch/cgsb/gresham/Gunjan/oe
#PBS -e /scratch/cgsb/gresham/Gunjan/oe
#PBS -l nodes=1:ppn=20
#PBS -l mem=25GB
#PBS -l walltime=48:00:00
#PBS -M ggg256@nyu.edu
#PBS -t 1-100


# variable used in script
type="deletions"
folder="100x"  
read_group="100x"

#paths to directories
RUNDIR=/scratch/cgsb/gresham/Gunjan/${type}/${folder}
ref="/scratch/work/cgsb/reference_genomes/Public/Fungi/Saccharomyces_cerevisiae/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.fna"

cd $RUNDIR/sim${PBS_ARRAYID}

fastq1=$RUNDIR/sim${PBS_ARRAYID}/sim${PBS_ARRAYID}_1.fq
fastq2=$RUNDIR/sim${PBS_ARRAYID}/sim${PBS_ARRAYID}_2.fq

#############################################################################

module purge
module load bwa/gnu/0.7.13
module load samtools/intel/1.3

bwa mem -t 20 $ref $fastq1 $fastq2 > sim${PBS_ARRAYID}.sam
samtools view -bS sim${PBS_ARRAYID}.sam > sim${PBS_ARRAYID}.bam
samtools sort sim${PBS_ARRAYID}.bam > sim${PBS_ARRAYID}.sorted.bam
samtools index sim${PBS_ARRAYID}.sorted.bam

module unload jdk
module load picard-tools/1.129
# obtaining alignment metrics using Picards tools

# obtaining alignment metrics using Picards tools
java -jar $PICARD_JAR \
CollectAlignmentSummaryMetrics \
R=$ref \
I=sim${PBS_ARRAYID}.sorted.bam \
O=sim${PBS_ARRAYID}_alignment_metrics.txt 

# obtaining insert size metrics using Picards tools
java -jar $PICARD_JAR \
CollectInsertSizeMetrics \
INPUT=sim${PBS_ARRAYID}.sorted.bam  \
OUTPUT=sim${PBS_ARRAYID}_insert_metrics.txt \
HISTOGRAM_FILE=sim${PBS_ARRAYID}_insert_size_histogram.pdf 

# obtaining read depth ie coverage using samtools
module load samtools/intel/1.3
samtools depth -a sim${PBS_ARRAYID}.sorted.bam > sim${PBS_ARRAYID}_RD.txt

# removing duplicates from the sorted bam file and building index using picard
java -jar $PICARD_JAR \
MarkDuplicates \
INPUT=sim${PBS_ARRAYID}.sorted.bam  \
OUTPUT=sim${PBS_ARRAYID}_rm_dup.bam \
METRICS_FILE=sim${PBS_ARRAYID}_rmdup_metrics.txt \
REMOVE_DUPLICATES=true

java -jar $PICARD_JAR \
BuildBamIndex \
INPUT=sim${PBS_ARRAYID}_rm_dup.bam

# assigning to bam(variable) the sorted bam file to be used as input while running algortihms 
bam1=sim${PBS_ARRAYID}_rm_dup.bam
bam=sim${PBS_ARRAYID}.sorted.bam

# obtain config file for pindel
module purge
module load breakdancer/intel/1.1.2
bam2cfg.pl $bam -h > sim${PBS_ARRAYID}_bd.cfg
mean_insert_size="$(less sim${PBS_ARRAYID}_bd.cfg | cut -f9)"
mean_IS="$echo `expr substr $mean_insert_size 6 8`" 
echo "${bam} ${mean_IS} sim${PBS_ARRAYID}" > config_sim${PBS_ARRAYID}.txt

module purge
module load pindel/intel/0.2.5a4 
/share/apps/pindel/0.2.5a4/intel/bin/pindel \
-T 20 \
-f $ref \
-i config_sim${PBS_ARRAYID}.txt \
-c ALL \
-o sim${PBS_ARRAYID}_output

module load ipython
python /home/ggg256/scripts/parse_pindel_D_INV_TD_SI.py -f sim${PBS_ARRAYID}_output_D > sim${PBS_ARRAYID}_pindel.txt
python /home/ggg256/scripts/parse_pindel_D_INV_TD_SI.py -f sim${PBS_ARRAYID}_output_TD >> sim${PBS_ARRAYID}_pindel.txt

echo "pindel all done"

#############
# obtaining read depth ie coverage to decide bin size while running cnvnator algorithm
module load r/intel/3.3.1
Rscript /home/ggg256/scripts/read_depth_for_bin_size.R -r sim${PBS_ARRAYID}_RD.txt \
> sim${PBS_ARRAYID}_readDepth.txt
bin_size="$(cat sim${PBS_ARRAYID}_readDepth.txt|replace \" ""|replace [1] "" )"

# obtaining individual fasta files from reference file in the "same directory"
# this step is very important for cnvnator to work
python /home/ggg256/scripts/fasta_to_each_chr.py

# change individual fasta file names as per CNVnator requirements.

mv NC_001133.9*fa NC_001133.9.fa
mv NC_001134.8*fa NC_001134.8.fa
mv NC_001135.5*fa NC_001135.5.fa
mv NC_001136.10*fa NC_001136.10.fa
mv NC_001137.3*fa NC_001137.3.fa
mv NC_001138.5*fa NC_001138.5.fa
mv NC_001139.9*fa NC_001139.9.fa
mv NC_001140.6*fa NC_001140.6.fa
mv NC_001141.2*fa NC_001141.2.fa
mv NC_001142.9*fa NC_001142.9.fa
mv NC_001143.9*fa NC_001143.9.fa
mv NC_001144.5*fa NC_001144.5.fa
mv NC_001145.3*fa NC_001145.3.fa
mv NC_001146.8*fa NC_001146.8.fa
mv NC_001147.6*fa NC_001147.6.fa
mv NC_001148.4*fa NC_001148.4.fa
mv NC_001224.1*fa NC_001224.1.fa

module purge
module load cnvnator/intel/0.3.2

cnvnator \
-root sim${PBS_ARRAYID}_out.root \
-genome $ref \
-tree $bam1 \
-unique

cnvnator \
-root sim${PBS_ARRAYID}_out.root \
-genome $ref \
-tree $bam1 \
-his ${bin_size}

cnvnator \
-root sim${PBS_ARRAYID}_out.root \
-genome $ref \
-tree $bam1 \
-stat ${bin_size}

cnvnator \
-root sim${PBS_ARRAYID}_out.root \
-genome $ref \
-tree $bam1 \
-partition ${bin_size}

cnvnator \
-root sim${PBS_ARRAYID}_out.root \
-genome $ref \
-tree $bam1 \
-call ${bin_size} > sim${PBS_ARRAYID}_cnvnator.txt

module load ipython
python /home/ggg256/scripts/parse_cnvnator.py -f sim${PBS_ARRAYID}_cnvnator.txt -t deletion > sim${PBS_ARRAYID}_cnv.txt
python /home/ggg256/scripts/parse_cnvnator.py -f sim${PBS_ARRAYID}_cnvnator.txt -t duplication >> sim${PBS_ARRAYID}_cnv.txt

# deleting the individual fasta files to save space
rm *fa
echo "cnvnator done"

#######
module purge
module load samtools/intel/1.3
module load bamaddrg

#obtaining discordants and split reads
samtools view -b -F 1294 $bam > discordants.bam
samtools view -h $bam|/share/apps/lumpy/0.2.13/intel/scripts/extractSplitReads_BwaMem -i stdin| samtools view -Sb - > splitters.bam
echo "splitters and discordants extracted"

# adding read groups in bam file(s)
bamaddrg -b $bam -s ${read_group} > RG.bam
bamaddrg -b splitters.bam -s ${read_group} > RG.splitters.bam
bamaddrg -b discordants.bam -s ${read_group} > RG.discordants.bam
echo "bamaddrg done"

# sort and index bam files
samtools sort RG.bam > RG.sorted.bam
samtools sort RG.splitters.bam > RG.splitters.sorted.bam
samtools sort RG.discordants.bam > RG.discordants.sorted.bam
samtools index RG.sorted.bam
samtools index RG.splitters.sorted.bam
samtools index RG.discordants.sorted.bam
echo "lumpy sorting and indexing done"

module purge
module load lumpy/intel/0.2.13
/share/apps/lumpy/0.2.13/intel/scripts/lumpyexpress \
-B RG.sorted.bam \
-S RG.splitters.sorted.bam \
-D RG.discordants.sorted.bam \
-o sim${PBS_ARRAYID}_lumpy.vcf

module load ipython
python  /home/ggg256/scripts/parse_lumpy.py -f sim${PBS_ARRAYID}_lumpy.vcf > sim${PBS_ARRAYID}_lumpy.txt
echo "lumpy done"

####
# lumpy.sorted.bam is just  sorted bam file with RG and can be used as an input for svaba
# it is important to include read groups for svaba to run
module purge
module load svaba/intel/20170210 
svaba run -p 20 -G $ref -t RG.sorted.bam
gunzip -c no_id.alignments.txt.gz | grep contig_name > ${ID}_plot.txt
mv no_id.svaba.sv.vcf sim${PBS_ARRAYID}_svaba.vcf

module load ipython
python /home/ggg256/scripts/parse_svaba.py -f sim${PBS_ARRAYID}_svaba.vcf > sim${PBS_ARRAYID}_svaba.txt
echo "svaba done"

# Removing big size unnecessary files for this simulation study to save on memory
rm *fq
rm *fasta
rm no_id*
rm sim${PBS_ARRAYID}_output_*
rm sim${PBS_ARRAYID}_bd_output
rm sim${PBS_ARRAYID}_out.root
rm *bam
rm *sam

echo "ALL DONE"

exit 0;
